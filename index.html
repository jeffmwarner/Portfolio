<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Symptom Triage</title>
<link rel="stylesheet" type="text/css" href="/Portfolio/styles.css">
</head>
<script src="/Portfolio/scripts/hypertension.js"></script>
<script src="/Portfolio/scripts/mh_intake.js"></script>

<!-- <h2>Try a Sample Mental Health Intake Prompt</h2>
<div>
  <textarea id="userInput" rows="4" placeholder="Tell me a bit about how you're feeling today..."></textarea>
  <br />
  <button id="sendRealBtn">Ask the assistant</button>
</div>
<pre id="realAssistantOutput" class="assistant-output"></pre> -->

<h2>Scripted Conversation Flow Demo</h2>
<p>
  This example shows different scripted flows. Choose one to try:
</p>

<label for="scenarioSelect"><strong>Choose a scenario:</strong></label>
<select id="scenarioSelect">
  <option value="hypertension">Hypertension education</option>
  <option value="mh_intake">Mental health intake (therapy)</option>
</select>

<div id="scripted-chat" class="chat-container">
  <div id="chat-log"></div>
  <div id="chat-input-area" class="chat-input-area"></div>
</div>

<script>
  let conversationScript = [];
  let activeLLMProfile = 'default';
  const API_URL = "https://conversation-backend-yxsq.onrender.com/api/chat"; // my backend URL at Render

  document.getElementById("sendRealBtn").addEventListener("click", async () => {
    const inputEl = document.getElementById("userInput");
    const outputEl = document.getElementById("realAssistantOutput");
    const text = inputEl.value.trim();

    if (!text) {
      outputEl.textContent = "Please type something so I can send it to the assistant.";
      return;
    }

    outputEl.textContent = "...";

    try {
      const res = await fetch(API_URL, {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({ message: text })
      });

      if (!res.ok) {
        outputEl.textContent = "The assistant ran into a problem. Please try again later.";
        return;
      }

      const data = await res.json();

      const formatReply = (text) => {
        return text.replace(/([.?!])\s+/g, '$1\n\n');
      }
      
      outputEl.textContent = data.reply;
    } catch (err) {
      console.error(err);
      
      outputEl.textContent = "Network error talking to the assistant.";
    }
  });
</script>
<script>
  

  // --- Predetermined conversation script with optional routing fields ---


const LLM_PROFILES = {
  default: `
You are a careful, concise assistant. Be clear, kind, and avoid medical or legal advice.
`.trim(),

  hypertensionEducation: `
You are a conservative, safety-focused assistant explaining hypertension (high blood pressure)
to patients in plain language. You:
- Do NOT diagnose, prescribe, or adjust medications.
- Encourage following the patient’s own clinician's advice.
- Explain concepts like blood pressure ranges, home monitoring, and when to seek urgent/emergency care.
- Always include a brief safety disclaimer.
Keep responses short, structured, and reassuring.
`.trim(),

  mentalHealthIntake: `
You are a supportive mental-health intake assistant.
Your job is to:
- Reflect and validate what the person shares.
- Ask gentle follow-up questions if needed.
- Avoid diagnoses or labels.
- If someone mentions self-harm, harm to others, or feeling unsafe, gently encourage crisis resources or emergency care.
Keep responses warm, human, and 3–5 sentences long.
`.trim(),
};

const SCENARIOS = {
  hypertension: {
      script: SCRIPT_HYPERTENSION,
      llmProfile: "hypertensionEducation",
  },
  mh_intake: {
      script: SCRIPT_MH_INTAKE,
      llmProfile: "mentalHealthIntake",
  }
};
    
    

const STATICconversationScript = [
  {
    type: "bot",
    stepId: "intro1",
    next: "intro2",
    text: `Hi there! I'm here to help you get connected with care.`,
  },
  {
    type: "bot",
    stepId: "intro2",
    next: "askConcern",
    text: `To get started`,
  },
  {
    type: "choice",
    stepId: "focusChoice",
    branches: { "Stress": "stressPath", "Sleep": "sleepPath", "Depression": "depressionPath", "Anxiety": "anxietyPath", "Skip this": "final" },
    id: "focusArea",
    prompt: `Which area feels most important to talk about first?`,
    choices: ["Stress", "Sleep", "Depression", "Anxiety"],
  },
  {
    type: "bot",
    stepId: "stressPath",
    next: "stressLLM",
    text: `Let's talk through stress first.`,
  },
  {
    type: "llmBot",
    stepId: "stressLLM",
    next: "rejoin",
    source: "focusArea",
    endpoint: "https://conversation-backend-yxsq.onrender.com/api/chat",
  },
  {
    type: "bot",
    stepId: "sleepPath",
    next: "sleepLLM",
    text: `Sleep problems can impact everything. Let's unpack that.`,
  },
  {
    type: "llmBot",
    stepId: "sleepLLM",
    next: "rejoin",
    source: "focusArea",
    endpoint: "https://conversation-backend-yxsq.onrender.com/api/chat",
  },
  {
    type: "bot",
    stepId: "depressionPath",
    next: "depLLM",
    text: `Thanks for bringing up depression, that's a really important topic.`,
  },
  {
    type: "llmBot",
    stepId: "depLLM",
    next: "rejoin",
    source: "focusArea",
    endpoint: "https://conversation-backend-yxsq.onrender.com/api/chat",
  },
  {
    type: "bot",
    stepId: "anxietyPath",
    next: "anxietyLLM",
    text: `While anxiety can be helpful in the right situation, too much anxiety at the wrong time can be a lot to manage.`,
  },
  {
    type: "llmBot",
    stepId: "anxietyLLM",
    next: "rejoin",
    source: "focusArea",
  },
  {
    type: "bot",
    stepId: "rejoin",
    next: "nextStep",
    text: `Before we wrap up`,
  },
  {
    type: "choice",
    stepId: "nextStep",
    next: "final",
    id: "nextStepChoice",
    prompt: `What feels like a helpful next step?`,
    choices: ["Get matched with a therapist", "Learn what therapy is like", "Try coping strategies"],
  },
  {
    type: "bot",
    stepId: "final",
    text: `Thanks! This is only a short demo, but this framework scales to a full intake experience.`,
  },
];

const AAAconversationScript = [
    {
      type: "bot",
      stepId: "intro1",
      text: "Hello! I'm your digital assistant, here to help with your recent hypertension (high blood pressure) diagnosis.",
      next: "intro2",
    },
    {
      type: "bot",
      stepId: "intro2",
      text: "There's a lot of things to cover, and it can feel overwhelming...",
      next: "intro3",
    },
    {
      type: "bot",
      stepId: "intro3",
      text: "...but that's where we can help!",
      next: "focusChoice",
    },
    {
      type: "choice",
      stepId: "focusChoice",
      id: "focusArea",
      prompt:
        "If you had to choose one area to focus on first, which feels most important?",
      choices: [
        "Signs and symptoms",
        "Measuring blood pressure",
        "Medication concerns",
      ],
      // You can branch each choice to its own path later if you want.
      // For now, we'll just send whatever they pick to the LLM.
      next: "focusLLM",
    },
    {
      type: "llmBot",
      stepId: "focusLLM",
      source: "focusArea", // use the user's choice as input to the LLM
      // id: "focusExplanation", // optional: store LLM reply in state
      // endpoint: "/api/chat",   // optional override (defaults to API_URL)
    },
  ];
const scenarioSelect = document.getElementById("scenarioSelect");

scenarioSelect.addEventListener("change", (e) => {
  const key = e.target.value;
  resetConversation(key);
});


function resetConversation(key) {
  const scenario = SCENARIOS[key];
  if (!scenario) return;

  conversationScript = scenario.script;
  activeLLMProfile = scenario.llmProfile || "default";

  // reset UI + state
  chatLogEl.innerHTML = "";
  chatInputAreaEl.innerHTML = "";
  currentStepIndex = 0;
  for (const k of Object.keys(state)) delete state[k];

  renderStep();
}


  // --- Build stepId -> index map for branching ---
  const stepIndexById = {};
  conversationScript.forEach((step, index) => {
    if (step.stepId) {
      stepIndexById[step.stepId] = index;
    }
  });

  function getNextIndex(currentIndex, step, options = {}) {
    const { choiceValue } = options;

    // 1. If this is a choice with branches and we have a match, follow that
    if (step.branches && choiceValue && step.branches[choiceValue]) {
      const target = step.branches[choiceValue];
      if (typeof target === "number") return target;
      if (typeof target === "string" && stepIndexById[target] != null) {
        return stepIndexById[target];
      }
    }

    // 2. Otherwise, if step has a 'next', follow it
    if (step.next != null) {
      if (typeof step.next === "number") {
        return step.next;
      }
      if (typeof step.next === "string" && stepIndexById[step.next] != null) {
        return stepIndexById[step.next];
      }
    }

    // 3. Fallback: go to the next step in sequence
    return currentIndex + 1;
  }

  // --- DOM wiring ---

  const chatLogEl = document.getElementById("chat-log");
  const chatInputAreaEl = document.getElementById("chat-input-area");

  let currentStepIndex = 0;
  const state = {};

  function addBubble(text, role) {
    const bubble = document.createElement("div");
    bubble.className = `chat-bubble ${role}`;
    bubble.textContent = text;
    chatLogEl.appendChild(bubble);
    chatLogEl.scrollTop = chatLogEl.scrollHeight;
  }

  async function callLLM(userText, options = {}) {
  const { endpoint, systemPrompt } = options;
  const url = endpoint || API_URL;

  const response = await fetch(url, {
    method: "POST",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify({
      message: userText,
      systemPrompt: systemPrompt || undefined, // omit if not provided
    }),
  });

  if (!response.ok) {
    console.error("LLM error:", await response.text());
    return "Sorry, I’m having trouble responding right now.";
  }

  const data = await response.json();
  return data.reply || data.summary || JSON.stringify(data);
}

    
  async function XXXcallLLM(userText, endpointOverride) {
    const url = endpointOverride || API_URL;

    const response = await fetch(url, {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify({ message: userText }),
    });

    if (!response.ok) {
      console.error("LLM error:", await response.text());
      return "Sorry, I’m having trouble responding right now.";
    }

    const data = await response.json();
    return data.reply || data.summary || JSON.stringify(data);
  }

  // Typing indicator

  let typingEl = null;

  function showTypingIndicator(text = "") {
    if (typingEl) return; // avoid duplicates

    typingEl = document.createElement("div");
    typingEl.className = "chat-bubble bot";
    typingEl.innerHTML = `
      <div class="typing-indicator">
        <span>${text}</span>
        <div class="typing-dots">
          <span></span><span></span><span></span>
        </div>
      </div>
    `;
    chatLogEl.appendChild(typingEl);
    chatLogEl.scrollTop = chatLogEl.scrollHeight;
  }

  function hideTypingIndicator() {
    if (typingEl && typingEl.parentNode) {
      typingEl.parentNode.removeChild(typingEl);
    }
    typingEl = null;
  }

  // --- Main step renderer with branching ---

  function renderStep() {
    chatInputAreaEl.innerHTML = ""; // clear input area

    if (currentStepIndex >= conversationScript.length) {
      return;
    }

    const step = conversationScript[currentStepIndex];

    // BOT
    if (step.type === "bot") {
      const raw =
        typeof step.text === "function" ? step.text(state) : step.text;

      showTypingIndicator();
      chatInputAreaEl.innerHTML = "";

      setTimeout(() => {
        hideTypingIndicator();
        addBubble(raw, "bot");
        const nextIndex = getNextIndex(currentStepIndex, step);
        currentStepIndex = nextIndex;
        setTimeout(renderStep, 500);
      }, 500);
    }

    // USER INPUT
    else if (step.type === "userInput") {
      addBubble(step.prompt, "bot");

      const wrapper = document.createElement("div");
      const textarea = document.createElement("textarea");
      textarea.rows = 3;
      textarea.placeholder = "Type your response here...";

      const submitBtn = document.createElement("button");
      submitBtn.textContent = "Submit";
      submitBtn.className = "primary";

      submitBtn.addEventListener("click", () => {
        const value = textarea.value.trim();
        if (!value) return;

        addBubble(value, "user");
        state[step.id] = value;

        chatInputAreaEl.innerHTML = "";
        const nextIndex = getNextIndex(currentStepIndex, step);
        currentStepIndex = nextIndex;
        renderStep();
      });

      wrapper.appendChild(textarea);
      wrapper.appendChild(submitBtn);
      chatInputAreaEl.appendChild(wrapper);
    }

    // CHOICE
    else if (step.type === "choice") {
      addBubble(step.prompt, "bot");

      const wrapper = document.createElement("div");
      wrapper.className = "choice-buttons";

      step.choices.forEach((choiceText) => {
        const btn = document.createElement("button");
        btn.type = "button";
        btn.textContent = choiceText;

        btn.addEventListener("click", () => {
          addBubble(choiceText, "user");
          state[step.id] = choiceText;

          chatInputAreaEl.innerHTML = "";
          const nextIndex = getNextIndex(currentStepIndex, step, {
            choiceValue: choiceText,
          });
          currentStepIndex = nextIndex;
          renderStep();
        });

        wrapper.appendChild(btn);
      });

      chatInputAreaEl.appendChild(wrapper);
    }

    // LLM INPUT (free-text step that calls LLM immediately)
    else if (step.type === "llmInput") {
  addBubble(step.prompt, "bot");

  const wrapper = document.createElement("div");
  const textarea = document.createElement("textarea");
  textarea.rows = 3;
  textarea.placeholder = "Type your response here...";

  const submitBtn = document.createElement("button");
  submitBtn.textContent = "Submit";
  submitBtn.className = "primary";

  submitBtn.addEventListener("click", async () => {
    const value = textarea.value.trim();
    if (!value) return;

    addBubble(value, "user");
    state[step.id] = value;
    chatInputAreaEl.innerHTML = "";

    showTypingIndicator("Thinking about your response…");

    const profileKey = step.llmProfile || activeLLMProfile || "default";
    const systemPrompt = LLM_PROFILES[profileKey];

    const llmReply = await callLLM(value, {
      endpoint: step.endpoint,
      systemPrompt,
    });

    hideTypingIndicator();
    addBubble(llmReply, "bot");

    const nextIndex = getNextIndex(currentStepIndex, step);
    currentStepIndex = nextIndex;
    renderStep();
  });

  wrapper.appendChild(textarea);
  wrapper.appendChild(submitBtn);
  chatInputAreaEl.appendChild(wrapper);
}

    // LLM BOT (auto-call LLM using previous state[source])
    else if (step.type === "llmBot") {
  const sourceId = step.source;
  const inputText = state[sourceId];

  if (!inputText) {
    addBubble("I don’t have enough information yet to respond.", "bot");
    const nextIndex = getNextIndex(currentStepIndex, step);
    currentStepIndex = nextIndex;
    setTimeout(renderStep, 500);
    return;
  }

  chatInputAreaEl.innerHTML = "";
  showTypingIndicator();

  (async () => {
    const profileKey = step.llmProfile || activeLLMProfile || "default";
    const systemPrompt = LLM_PROFILES[profileKey];

    const llmReply = await callLLM(inputText, {
      endpoint: step.endpoint,
      systemPrompt,
    });

    hideTypingIndicator();
    addBubble(llmReply, "bot");

    if (step.id) {
      state[step.id] = llmReply;
    }

    const nextIndex = getNextIndex(currentStepIndex, step);
    currentStepIndex = nextIndex;
    renderStep();
  })();
}

  }

  // Kick off the scripted conversation
  //renderStep();
    resetConversation("hypertension");
</script>



</html>
