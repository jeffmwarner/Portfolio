<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Intelligent Health Conversation</title>
  <link rel="stylesheet" type="text/css" href="/Portfolio/styles.css">
</head>
<script src="/Portfolio/scripts/hypertension.js"></script>
<script src="/Portfolio/scripts/mh_intake.js"></script>

<!-- <h2>Try a Sample Mental Health Intake Prompt</h2>
<div>
  <textarea id="userInput" rows="4" placeholder="Tell me a bit about how you're feeling today..."></textarea>
  <br />
  <button id="sendRealBtn">Ask the assistant</button>
</div>
<pre id="realAssistantOutput" class="assistant-output"></pre> -->
<div id="top-bar">
<h2>Hybrid Scripted/LLM Conversation Flow Demo</h2>
<p>
  This example shows different conversation flows featuring scripted content and LLM interactivity. Choose one to try:
  <br><br> ... OR try <button id="script-builder-btn">building your own</button>
</p>

<label for="scenarioSelect"><strong>Choose a scenario:</strong></label>
<select id="scenarioSelect">
  <option value=""></option>
  <option value="hypertension">Hypertension education</option>
  <option value="mh_intake">Mental health intake (therapy)</option>
</select>
</div>
<button id="headerToggle" class="header-toggle" aria-label="Show header">
  Hide options <!-- ⌄ Options -->
</button>
<div id="scripted-chat" class="chat-container">
  <div id="chat-log"></div>
  <div id="chat-input-area" class="chat-input-area"></div>
</div>

<script>
  let conversationScript = [];
  let stepIndexById = {};

  function rebuildStepIndex() {
    stepIndexById = {};
    conversationScript.forEach((step, index) => {
      if (step.stepId) {
        stepIndexById[step.stepId] = index;
      }
    });
  }

  const chatContainerEl = document.getElementById("scripted-chat");

  let activeLLMProfile = 'default';
  const API_URL = "https://conversation-backend-yxsq.onrender.com/api/chat"; // my backend URL at Render
  
  const LLM_PROFILES = {
    default: `
You are a careful, concise assistant. Be clear and kind. Avoid medical, legal,
or financial advice. If you are unsure, say so.
`.trim(),

    hypertensionEducation: `
You are an educational assistant explaining high blood pressure (hypertension)
in plain, patient-friendly language.

AUTHORITATIVE SOURCES
- Only provide information that is consistent with guidance from:
  - American Heart Association (AHA)
  - National Institutes of Health (NIH)
  - Centers for Disease Control and Prevention (CDC)
  - World Health Organization (WHO)

SAFETY AND SCOPE
- Do NOT diagnose, prescribe, or adjust medications.
- Do NOT give personalized treatment plans.
- Encourage the user to follow their own clinician’s advice.
- If the user describes very high blood pressure or red-flag symptoms
  (e.g., chest pain, trouble breathing, severe headache, confusion,
  weakness on one side, vision loss), recommend urgent or emergency care.

BEHAVIOR
- Use calm, reassuring, non-alarmist language.
- Explain terms simply (e.g., “top number” and “bottom number” for BP).
- Focus on education: what hypertension is, why it matters, and typical
  lifestyle and monitoring concepts.
- If the question goes beyond what AHA/NIH/CDC/WHO would reasonably cover,
  say you can’t answer and suggest talking with a clinician.

OUTPUT STYLE
- Use short paragraphs and simple lists with line breaks.
- No fancy markdown; plain text with line breaks is preferred.
- End each answer with a short safety reminder like:
  "This is general education, not personal medical advice."

SOURCE CHECK
- At the end of your answer, add a single line:
  "Source Check: Consistent with AHA/NIH/CDC educational guidance on hypertension."
`.trim(),

    mentalHealthIntake: `
You are a supportive mental-health intake assistant.

AUTHORITATIVE SOURCES
- Base your educational statements on guidance consistent with:
  - American Psychiatric Association (APA)
  - National Institutes of Health (NIH)
  - Centers for Disease Control and Prevention (CDC)
  - World Health Organization (WHO)

ROLE AND SCOPE
- Your main job is to:
  - Reflect and validate what the person shares.
  - Help clarify what they are hoping to work on.
  - Normalize help-seeking and explain what therapy or support might look like.
  - Explain in simple, general terms what treatment options may be commonly used to treat depression
- Do NOT diagnose mental health conditions.
- Do NOT recommend or adjust medications.
- If the user mentions self-harm, suicidal thinking, harm to others,
  feeling unsafe, or being in immediate danger:
  - Respond with high empathy.
  - Encourage contacting crisis support (e.g., 988 in the U.S. or local
    emergency services) or going to the nearest emergency department.

BEHAVIOR
- Use warm, respectful, inclusive language.
- Avoid clinical jargon; explain concepts in everyday terms.
- Keep answers focused and 3–6 sentences long unless the user explicitly
  asks for more detail.
- If the question goes beyond what APA/NIH/CDC/WHO would reasonably cover,
  gently say that you can’t answer fully and encourage them to speak with
  a licensed mental health professional.

OUTPUT STYLE
- Use short paragraphs with line breaks for readability.
- You may use very simple lists if helpful, but no heavy markdown.
- Always end with a brief safety/context line, such as:
  "This is general information and not a diagnosis or treatment plan."

SOURCE CHECK
- At the end of each answer, add one line:
  "Source Check: Consistent with APA/NIH/CDC/WHO educational guidance on mental health."
`.trim(),
  };


  const SCENARIOS = {
    hypertension: {
      script: SCRIPT_HYPERTENSION,
      llmProfile: "hypertensionEducation",
    },
    mh_intake: {
      script: SCRIPT_MH_INTAKE,
      llmProfile: "mentalHealthIntake",
    }
  };



  const STATICconversationScript = [
    {
      type: "bot",
      stepId: "intro1",
      next: "intro2",
      text: `Hi there! I'm here to help you get connected with care.`,
    },
    {
      type: "bot",
      stepId: "intro2",
      next: "askConcern",
      text: `To get started`,
    },
    {
      type: "choice",
      stepId: "focusChoice",
      branches: { "Stress": "stressPath", "Sleep": "sleepPath", "Depression": "depressionPath", "Anxiety": "anxietyPath", "Skip this": "final" },
      id: "focusArea",
      prompt: `Which area feels most important to talk about first?`,
      choices: ["Stress", "Sleep", "Depression", "Anxiety"],
    },
    {
      type: "bot",
      stepId: "stressPath",
      next: "stressLLM",
      text: `Let's talk through stress first.`,
    },
    {
      type: "llmBot",
      stepId: "stressLLM",
      next: "rejoin",
      source: "focusArea",
      endpoint: "https://conversation-backend-yxsq.onrender.com/api/chat",
    },
    {
      type: "bot",
      stepId: "sleepPath",
      next: "sleepLLM",
      text: `Sleep problems can impact everything. Let's unpack that.`,
    },
    {
      type: "llmBot",
      stepId: "sleepLLM",
      next: "rejoin",
      source: "focusArea",
      endpoint: "https://conversation-backend-yxsq.onrender.com/api/chat",
    },
    {
      type: "bot",
      stepId: "depressionPath",
      next: "depLLM",
      text: `Thanks for bringing up depression, that's a really important topic.`,
    },
    {
      type: "llmBot",
      stepId: "depLLM",
      next: "rejoin",
      source: "focusArea",
      endpoint: "https://conversation-backend-yxsq.onrender.com/api/chat",
    },
    {
      type: "bot",
      stepId: "anxietyPath",
      next: "anxietyLLM",
      text: `While anxiety can be helpful in the right situation, too much anxiety at the wrong time can be a lot to manage.`,
    },
    {
      type: "llmBot",
      stepId: "anxietyLLM",
      next: "rejoin",
      source: "focusArea",
    },
    {
      type: "bot",
      stepId: "rejoin",
      next: "nextStep",
      text: `Before we wrap up`,
    },
    {
      type: "choice",
      stepId: "nextStep",
      next: "final",
      id: "nextStepChoice",
      prompt: `What feels like a helpful next step?`,
      choices: ["Get matched with a therapist", "Learn what therapy is like", "Try coping strategies"],
    },
    {
      type: "bot",
      stepId: "final",
      text: `Thanks! This is only a short demo, but this framework scales to a full intake experience.`,
    },
  ];

  const AAAconversationScript = [
    {
      type: "bot",
      stepId: "intro1",
      text: "Hello! I'm your digital assistant, here to help with your recent hypertension (high blood pressure) diagnosis.",
      next: "intro2",
    },
    {
      type: "bot",
      stepId: "intro2",
      text: "There's a lot of things to cover, and it can feel overwhelming...",
      next: "intro3",
    },
    {
      type: "bot",
      stepId: "intro3",
      text: "...but that's where we can help!",
      next: "focusChoice",
    },
    {
      type: "choice",
      stepId: "focusChoice",
      id: "focusArea",
      prompt:
        "If you had to choose one area to focus on first, which feels most important?",
      choices: [
        "Signs and symptoms",
        "Measuring blood pressure",
        "Medication concerns",
      ],
      // Maybe branch each choice to its own path?
      // Currently just send whatever they pick to the LLM.
      next: "focusLLM",
    },
    {
      type: "llmBot",
      stepId: "focusLLM",
      source: "focusArea", // use the user's choice as input to the LLM
      // id: "focusExplanation", // optional: store LLM reply in state
      // endpoint: "/api/chat",   // optional override (defaults to API_URL)
    },
  ];

  function scrollChatToBottom() {
    requestAnimationFrame(() => {
      const container = document.getElementById("scripted-chat");
      if (!container) return;
      // container.scrollTop = container.scrollHeight;
      container.scrollTo({
        top: container.scrollHeight,
        behavior: "smooth"
      });
    });
  }

  function resetConversation(key) {
    // Kill pending timeouts
    if (pendingTimeout) {
      clearTimeout(pendingTimeout);
      pendingTimeout = null;
    }

    runId++; // invalidate in-flight async calls

    // Reset UI
    chatLogEl.innerHTML = "";
    chatInputAreaEl.innerHTML = "";

    // Reset state
    state = {};
    currentStepIndex = 0;

    // Load scenario script + profile
    const scenario = SCENARIOS[key] || {};
    conversationScript = scenario.script || [];
    activeLLMProfile = scenario.llmProfile || "default";

    // rebuild stepId -> index map
    rebuildStepIndex();

    if (conversationScript.length > 0) {
      renderStep();
    }
  }

  const scenarioSelect = document.getElementById("scenarioSelect");
  const headerToggle = document.getElementById("headerToggle");

if (headerToggle) {
  headerToggle.addEventListener("click", () => {
    // If collapsed, expand; if expanded, collapse
    const isCollapsed = document.body.classList.contains("chat-header-collapsed");
    if (isCollapsed) {
      expandHeader();
      headerToggle.textContent = "Hide options";
    } else {
      collapseHeaderForChat();
      headerToggle.textContent = "⌄ Options";
    }
  });
}
  const byo = document.getElementById("script-builder-btn");
  byo.addEventListener("click", () => {
    window.open('script_builder.html', '_blank');
  })
  function collapseHeaderForChat() {
    document.body.classList.add("chat-header-collapsed");
    headerToggle.textContent = "⌄ Options";
  }

  function expandHeader() {
    document.body.classList.remove("chat-header-collapsed");
    headerToggle.textContent = "Hide options"
  }

  scenarioSelect.addEventListener("change", (e) => {
    const key = e.target.value;
    if (!key) return;
    resetConversation(key);


  // Auto-collapse on mobile after selecting a scenario
 if (window.innerWidth <= 768 || window.innerHeight <= 1000) {
    collapseHeaderForChat();
    if (headerToggle) {
      headerToggle.textContent = "⌄ Options";
    }
  }
});

// Initial load: start with header visible; you can choose to collapse here too if you want
if (scenarioSelect.value) {
  resetConversation(scenarioSelect.value);
  // Optionally:
  if (window.innerWidth <= 768) collapseHeaderForChat();
}

  function getNextIndex(currentIndex, step, options = {}) {
    const { choiceValue } = options;

    // 1. If this is a choice with branches and we have a match, follow that
    if (step.branches && choiceValue && step.branches[choiceValue]) {
      const target = step.branches[choiceValue];
      if (typeof target === "number") return target;
      if (typeof target === "string" && stepIndexById[target] != null) {
        return stepIndexById[target];
      }
    }

    // 2. Otherwise, if step has a 'next', follow it
    if (step.next != null) {
      if (typeof step.next === "number") {
        return step.next;
      }
      if (typeof step.next === "string" && stepIndexById[step.next] != null) {
        return stepIndexById[step.next];
      }
    }

    // 3. Fallback: go to the next step in sequence
    return currentIndex + 1;
  }

  // --- DOM wiring ---

  const chatLogEl = document.getElementById("chat-log");
  const chatInputAreaEl = document.getElementById("chat-input-area");

  let activeScript = null;
  let activeProfile = null;
  let currentStepIndex = 0;
  let state = {};
  let pendingTimeout = null;
  let runId = 0; // prevents stale LLM responses

  function addBubble(text, role) {
    const bubble = document.createElement("div");
    bubble.className = `chat-bubble ${role}`;
    bubble.textContent = text;

    chatLogEl.appendChild(bubble);

    scrollChatToBottom();
  }

async function callLLM(userText, options = {}) {
  const { endpoint, systemPrompt } = options;
  const url = endpoint || API_URL;

  const response = await fetch(url, {
    method: "POST",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify({
      message: userText,
      systemPrompt: systemPrompt || undefined
    }),
  });

  const data = await response.json();
  const raw = data.reply || data.summary || JSON.stringify(data);
  const formatted = formatLLMReply(raw);
  //return formatLLMReply(raw);
  return splitIntoBubbles(formatted);
}

function splitIntoBubbles(text) {
  if (!text) return [];

  // Paragraphs = chunks separated by blank lines
  const paragraphs = text
    .split(/\n\s*\n/g)
    .map(p => p.trim())
    .filter(Boolean);

  const bubbles = [];
  let currentGroup = [];
  let currentLen = 0;

  // Tweak this number to taste
  const MAX_CHARS_PER_BUBBLE = 550;

  paragraphs.forEach(p => {
    const isDisclaimer =
      /general (education|information).*not.*(diagnosis|treatment|medical advice)/i.test(p);
    const isSource =
      /^source\s*check[:]?/i.test(p) || /^source[s]?:/i.test(p);

    // Disclaimer / Source always get their own bubble
    if (isDisclaimer || isSource) {
      if (currentGroup.length) {
        bubbles.push(currentGroup.join("\n\n"));
        currentGroup = [];
        currentLen = 0;
      }
      bubbles.push(p);
      return;
    }

    // If adding this paragraph would make the bubble too long,
    // flush the current bubble and start a new one.
    if (currentLen > 0 && (currentLen + p.length > MAX_CHARS_PER_BUBBLE)) {
      bubbles.push(currentGroup.join("\n\n"));
      currentGroup = [];
      currentLen = 0;
    }

    currentGroup.push(p);
    currentLen += p.length;
  });

  if (currentGroup.length) {
    bubbles.push(currentGroup.join("\n\n"));
  }

  return bubbles;
}




function formatLLMReply(text) {
  if (!text) return "";

  let t = text.trim();

  // Normalize line endings
  t = t.replace(/\r\n/g, "\n");

  // Normalize bullets (if they show up)
  t = t.replace(/[\u2022•]\s*/g, "• ");
  t = t.replace(/\n?[-•]\s+/g, "\n• ");

  // Add blank lines before common disclaimer phrases
  t = t.replace(
    /(this is\s+general information[^.\n]*\.)/i,
    "\n\n$1"
  );
  t = t.replace(
    /(this information\s+is (only )?educational[^.\n]*\.)/i,
    "\n\n$1"
  );
  t = t.replace(
    /(not (a )?diagnosis[^.\n]*\.)/i,
    "\n\n$1"
  );

  // Add blank lines before source lines
  t = t.replace(/(source\s*check[:]?)/i, "\n\n$1");

  // Also handle explicit "safety note"/"disclaimer" if they appear
  t = t.replace(/(safety note|safety reminder|disclaimer)/gi, "\n\n$1");

  // Collapse 3+ newlines to exactly 2
  t = t.replace(/\n{3,}/g, "\n\n");

  return t;
}




  function XXXformatLLMReply(text) {
    if (!text) return "";

    let out = text;

    // Normalize newlines
    out = out.replace(/\r\n?/g, "\n");

    // If the model shoves bullets right after a colon:
    // "ranges: • Normal..." -> "ranges:\n\n• Normal..."
    out = out.replace(/:\s*([\-*•])\s*/g, ':\n\n$1 ');

    // Convert hyphen/asterisk bullets at the start of a line into •
    // "- something" or "* something" -> "• something"
    out = out.replace(/^[\-\*]\s+/gm, "• ");

    // If a bullet appears mid-line without a newline before it,
    // insert a line break: "mmHg • Normal" -> "mmHg\n• Normal"
    out = out.replace(/([^.\n])\s+•\s+/g, "$1\n• ");

    // Add a blank line before blocks of bullets if they follow text
    out = out.replace(/([^.:\n])\n(• )/g, "$1\n\n$2");

    // Collapse too many newlines (3+ → 2)
    out = out.replace(/\n{3,}/g, "\n\n");

    // Trim extra whitespace
    out = out.trim();

    return out;
  }

  // Typing indicator
  let typingEl = null;
  function showTypingIndicator(text = "") {
    if (typingEl) return; // avoid duplicates

    typingEl = document.createElement("div");
    typingEl.className = "chat-bubble bot";
    typingEl.innerHTML = `
      <div class="typing-indicator">
        <span>${text}</span>
        <div class="typing-dots">
          <span></span><span></span><span></span>
        </div>
      </div>
    `;
    chatLogEl.appendChild(typingEl);
    chatLogEl.scrollTop = chatLogEl.scrollHeight;
  }

  function hideTypingIndicator() {
    if (typingEl && typingEl.parentNode) {
      typingEl.parentNode.removeChild(typingEl);
    }
    typingEl = null;
  }

  // --- Main step renderer with branching ---

  function renderStep() {
    chatInputAreaEl.innerHTML = ""; // clear input area
    if (!conversationScript || conversationScript.length === 0) {
      return;
    }
    if (currentStepIndex >= conversationScript.length) {
      return;
    }

    //const step = conversationScript[currentStepIndex];
    const step = conversationScript[currentStepIndex];

    // BOT
    if (step.type === "bot") {
      const raw =
        typeof step.text === "function" ? step.text(state) : step.text;

      showTypingIndicator();
      chatInputAreaEl.innerHTML = "";

      setTimeout(() => {
        hideTypingIndicator();
        addBubble(raw, "bot");
        const nextIndex = getNextIndex(currentStepIndex, step);
        currentStepIndex = nextIndex;
        setTimeout(renderStep, 500);
      }, 500);
    }

    // USER INPUT
    else if (step.type === "userInput") {
      addBubble(step.prompt, "bot");

      const wrapper = document.createElement("div");
      const textarea = document.createElement("textarea");
      textarea.rows = 3;
      textarea.placeholder = "Type your response here...";

      const submitBtn = document.createElement("button");
      submitBtn.textContent = "Submit";
      submitBtn.className = "primary";

      submitBtn.addEventListener("click", () => {
        const value = textarea.value.trim();
        if (!value) return;

        addBubble(value, "user");
        state[step.id] = value;

        chatInputAreaEl.innerHTML = "";
        const nextIndex = getNextIndex(currentStepIndex, step);
        currentStepIndex = nextIndex;
        renderStep();
      });

      wrapper.appendChild(textarea);
      wrapper.appendChild(submitBtn);
      chatInputAreaEl.appendChild(wrapper);
      scrollChatToBottom();
    }

    // CHOICE
    else if (step.type === "choice") {
      addBubble(step.prompt, "bot");

      const wrapper = document.createElement("div");
      wrapper.className = "choice-buttons";

      step.choices.forEach((choiceText) => {
        const btn = document.createElement("button");
        btn.type = "button";
        btn.textContent = choiceText;

        btn.addEventListener("click", () => {
          addBubble(choiceText, "user");
          state[step.id] = choiceText;

          chatInputAreaEl.innerHTML = "";
          const nextIndex = getNextIndex(currentStepIndex, step, {
            choiceValue: choiceText,
          });
          currentStepIndex = nextIndex;
          renderStep();
        });

        wrapper.appendChild(btn);
      });

      chatInputAreaEl.appendChild(wrapper);
      scrollChatToBottom();
    }

    // LLM INPUT (free-text step that calls LLM immediately)
    else if (step.type === "llmInput") {
      addBubble(step.prompt, "bot");

      const wrapper = document.createElement("div");
      const textarea = document.createElement("textarea");
      textarea.rows = 3;
      textarea.placeholder = "Type your response here...";

      const submitBtn = document.createElement("button");
      submitBtn.textContent = "Submit";
      submitBtn.className = "primary";

      submitBtn.addEventListener("click", async () => {
        const value = textarea.value.trim();
        if (!value) return;

        addBubble(value, "user");
        state[step.id] = value;
        chatInputAreaEl.innerHTML = "";

        showTypingIndicator("Thinking about your response…");

        const profileKey = step.llmProfile || activeLLMProfile || "default";
        const systemPrompt = LLM_PROFILES[profileKey];

        // const llmReply = await callLLM(value, {
        //   endpoint: step.endpoint,
        //   systemPrompt,
        // });
        // hideTypingIndicator();
        // addBubble(llmReply, "bot");

        const llmReply = await callLLM(value, { endpoint: step.endpoint, systemPrompt });
        hideTypingIndicator();

        const chunks = Array.isArray(llmReply) ? llmReply : [llmReply];
        chunks.forEach(chunk => addBubble(chunk, "bot"));
        
        scrollChatToBottom();

        const nextIndex = getNextIndex(currentStepIndex, step);
        currentStepIndex = nextIndex;
        renderStep();
      });

      wrapper.appendChild(textarea);
      wrapper.appendChild(submitBtn);
      chatInputAreaEl.appendChild(wrapper);
      scrollChatToBottom();
    }

    // LLM BOT (auto-call LLM using previous state[source])
    else if (step.type === "llmBot") {
      const sourceId = step.source;
      const inputText = state[sourceId];

      if (!inputText) {
        addBubble("I don’t have enough information yet to respond.", "bot");
        const nextIndex = getNextIndex(currentStepIndex, step);
        currentStepIndex = nextIndex;
        setTimeout(renderStep, 500);
        return;
      }

      chatInputAreaEl.innerHTML = "";
      showTypingIndicator();

      (async () => {
        const profileKey = step.llmProfile || activeLLMProfile || "default";
        const systemPrompt = LLM_PROFILES[profileKey];

        // const llmReply = await callLLM(inputText, {
        //   endpoint: step.endpoint,
        //   systemPrompt,
        // });

        // hideTypingIndicator();
        // addBubble(llmReply, "bot");

        // if (step.id) {
        //   state[step.id] = llmReply;
        // }
        const llmReply = await callLLM(inputText, { endpoint: step.endpoint, systemPrompt });
        hideTypingIndicator();

        const chunks = Array.isArray(llmReply) ? llmReply : [llmReply];
        chunks.forEach(chunk => addBubble(chunk, "bot"));

        if (step.id) {
          state[step.id] = chunks.join("\n\n");
        }


        const nextIndex = getNextIndex(currentStepIndex, step);
        currentStepIndex = nextIndex;
        renderStep();
      })();
    }

  }

  // Kick off the scripted conversation
  //renderStep();
  //resetConversation("hypertension");
  resetConversation(document.getElementById("scenarioSelect").value);
</script>



</html>
